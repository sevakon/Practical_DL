{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seminar_nlp_0.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6yB8T6XO6-6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yandexdataschool/Practical_DL/blob/spring21/seminar09_embeddings_rnn/seminar_nlp_0.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uC4FMdxxwwx"
      },
      "source": [
        "## Seminar 1: Fun with Word Embeddings\n",
        "\n",
        "Today we gonna play with word embeddings: train our own little embedding, load one from   gensim model zoo and use it to visualize text corpora.\n",
        "\n",
        "This whole thing is gonna happen on top of embedding dataset.\n",
        "\n",
        "__Requirements:__  `pip install --upgrade nltk gensim bokeh` , but only if you're running locally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qKISkvuO5hS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "G-d99E0wxww4"
      },
      "source": [
        "# download the data:\n",
        "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt\n",
        "# alternative download link: https://yadi.sk/i/BPQrUu1NaTduEw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "NBw8BtUsxww5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
        "data[50]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbjPj0Jexww6"
      },
      "source": [
        "__Tokenization:__ a typical first step for an nlp task is to split raw data into words.\n",
        "The text we're working with is in raw format: with all the punctuation and smiles attached to some words, so a simple str.split won't do.\n",
        "\n",
        "Let's use __`nltk`__ - a library that handles many nlp tasks like tokenization, stemming or part-of-speech tagging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ucs1SX3xww6"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "print(tokenizer.tokenize(data[50]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyU51Tib6YSh"
      },
      "source": [
        "dd = [elem for elem in data if 'books' in elem]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HC0MVCsxxww6"
      },
      "source": [
        "# TASK: lowercase everything and extract tokens with tokenizer. \n",
        "# data_tok should be a list of lists of tokens for each line in data.\n",
        "\n",
        "data_tok = #YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gLi99g-Exww7"
      },
      "source": [
        "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "assert all(all(isinstance(tok, str) for tok in row) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
        "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
        "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBvaJaXIxww7"
      },
      "source": [
        "print([' '.join(row) for row in data_tok[:2]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxigSNSXxww7"
      },
      "source": [
        "__Word vectors:__ as the saying goes, there's more than one way to train word embeddings. There's Word2Vec and GloVe with different objective functions. Then there's fasttext that uses character-level models to train word embeddings. \n",
        "\n",
        "The choice is huge, so let's start someplace small: __gensim__ is another nlp library that features many vector-based models incuding word2vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xI0jHBWrxww8"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(data_tok, \n",
        "                 size=32,      # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5).wv  # define context as a 5-word window around the target word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyivHP_axww8"
      },
      "source": [
        "# now you can get word vectors !\n",
        "model.get_vector('anything')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p6UaCGqxww8"
      },
      "source": [
        "# or query similar words directly. Go play with it!\n",
        "model.most_similar('bread')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWs9bBsJxww9"
      },
      "source": [
        "### Using pre-trained model\n",
        "\n",
        "Took it a while, huh? Now imagine training life-sized (100~300D) word embeddings on gigabytes of text: wikipedia articles or twitter posts. \n",
        "\n",
        "Thankfully, nowadays you can get a pre-trained word embedding model in 2 lines of code (no sms required, promise)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cW-q0IIbxww9"
      },
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load('glove-twitter-100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRW1xYItxww9"
      },
      "source": [
        "model.most_similar(positive=[\"coder\", \"money\"], negative=[\"brain\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnhMMhasxww9"
      },
      "source": [
        "### Visualizing word vectors\n",
        "\n",
        "One way to see if our vectors are any good is to plot them. Thing is, those vectors are in 30D+ space and we humans are more used to 2-3D.\n",
        "\n",
        "Luckily, we machine learners know about __dimensionality reduction__ methods.\n",
        "\n",
        "Let's use that to plot 1000 most frequent words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S-Eq_6exww9"
      },
      "source": [
        "words = sorted(model.vocab.keys(), \n",
        "               key=lambda word: model.vocab[word].count,\n",
        "               reverse=True)[:1000]\n",
        "\n",
        "print(words[::100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9cmNGxUxww-"
      },
      "source": [
        "# for each word, compute it's vector with model\n",
        "word_vectors = # YOUR CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "s86wIUA0xww-"
      },
      "source": [
        "assert isinstance(word_vectors, np.ndarray)\n",
        "assert word_vectors.shape == (len(words), 100)\n",
        "assert np.isfinite(word_vectors).all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiGajGq2xww-"
      },
      "source": [
        "#### Linear projection: PCA\n",
        "\n",
        "The simplest linear dimensionality reduction method is __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "In geometric terms, PCA tries to find axes along which most of the variance occurs. The \"natural\" axes, if you wish.\n",
        "\n",
        "<img src=\"https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/pca_fish.png\" style=\"width:30%\">\n",
        "\n",
        "\n",
        "Under the hood, it attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
        "\n",
        "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
        "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
        "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
        "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
        "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VpWGMBxVxww-"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
        "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
        "word_vectors_pca = #YOUR CODE\n",
        "\n",
        "word_vectors_pca = (word_vectors_pca - word_vectors_pca.mean(axis=0)) / word_vectors_pca.std(axis=0)\n",
        "\n",
        "# and maybe MORE OF YOUR CODE here :)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppiui83G1_Um"
      },
      "source": [
        "word_vectors_pca.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "YZAF43PVxww_"
      },
      "source": [
        "assert word_vectors_pca.shape == (len(word_vectors), 2), \"there must be a 2d vector for each word\"\n",
        "assert max(abs(word_vectors_pca.mean(0))) < 1e-5, \"points must be zero-centered\"\n",
        "assert max(abs(1.0 - word_vectors_pca.std(0))) < 1e-2, \"points must have unit variance\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYceQG25xww_"
      },
      "source": [
        "#### Let's draw it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYhseQeRxww_"
      },
      "source": [
        "%pylab inline\n",
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    if isinstance(color, str): color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU0R8RhBxwxA"
      },
      "source": [
        "draw_vectors(word_vectors_pca[:, 0], word_vectors_pca[:, 1], token=words)\n",
        "\n",
        "# hover a mouse over there and see if you can identify the clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gM6Fjo_xwxA"
      },
      "source": [
        "### Visualizing neighbors with t-SNE\n",
        "PCA is nice but it's strictly linear and thus only able to capture coarse high-level structure of the data.\n",
        "\n",
        "If we instead want to focus on keeping neighboring points near, we could use TSNE, which is itself an embedding method. Here you can read __[more on TSNE](https://distill.pub/2016/misread-tsne/)__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3dQ0z_SxwxA"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# map word vectors onto 2d plane with TSNE. hint: don't panic it may take a minute or two to fit.\n",
        "# normalize them as just lke with pca\n",
        "\n",
        "\n",
        "word_tsne = #YOUR CODE\n",
        "word_tsne = (word_tsne - word_tsne.mean(axis=0))/word_tsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "z054y1ugxwxA"
      },
      "source": [
        "draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBnWIbH4xNou"
      },
      "source": [
        "## Generating names with recurrent neural networks\n",
        "\n",
        "This time you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
        "\n",
        "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead;\n",
        "\n",
        "It's dangerous to go alone, take these:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D88PlLssyY2Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtOprx8exNo4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# if you're in in colab, uncomment\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_RL/99ae2a3dae648428edbfc41fd10ed688e5365161/week07_%5Brecap%5D_rnn/names -O names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO7UJFP-xNpM"
      },
      "source": [
        "### Our data\n",
        "The dataset contains ~8k earthling names from different cultures, all in latin transcript.\n",
        "\n",
        "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkxTfJKBxNpT"
      },
      "source": [
        "import os\n",
        "start_token = \" \"\n",
        "\n",
        "with open(\"names\") as f:\n",
        "    lines = f.read()[:-1].split('\\n')\n",
        "    lines = [start_token + line for line in lines]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSUSTDKWxNph"
      },
      "source": [
        "print ('n samples = ',len(lines))\n",
        "for x in lines[::1000]:\n",
        "    print (x)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-ztLh0wxNp2"
      },
      "source": [
        "MAX_LENGTH = max(map(len, lines))\n",
        "print(\"max length =\", MAX_LENGTH)\n",
        "\n",
        "plt.title('Sequence length distribution')\n",
        "plt.hist(list(map(len, lines)),bins=25);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ab7zEqJxNp-"
      },
      "source": [
        "### Text processing\n",
        "\n",
        "First we need next to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpppLCJ8xNqC"
      },
      "source": [
        "#all unique characters go here\n",
        "tokens = # you code here <all unique characters in the dataset>\n",
        "\n",
        "\n",
        "num_tokens = len(tokens)\n",
        "print ('num_tokens = ', num_tokens)\n",
        "\n",
        "assert 50 < num_tokens < 60, \"Names should contain within 50 and 60 unique tokens depending on encoding\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rObMYUnxNqL"
      },
      "source": [
        "### Convert characters to integers\n",
        "\n",
        "Torch is built for crunching numbers, not strings. \n",
        "To train our neural network, we'll need to replace characters with their indices in tokens list.\n",
        "\n",
        "Let's compose a dictionary that does this mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFUPS7SZxNqP"
      },
      "source": [
        "token_to_id = # you code here <dictionary of symbol -> its identifier (index in tokens list)>\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBD3JmwLxNqd"
      },
      "source": [
        "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\"\n",
        "\n",
        "for i in range(num_tokens):\n",
        "    assert token_to_id[tokens[i]] == i, \"token identifier must be it's position in tokens list\"\n",
        "\n",
        "print(\"Seems alright!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AATQIEw-xNqw"
      },
      "source": [
        "def to_matrix(lines, max_len=None, pad=token_to_id[' '], dtype='int32', batch_first = True):\n",
        "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
        "    \n",
        "    max_len = max_len or max(map(len, lines))\n",
        "    lines_ix = np.zeros([len(lines), max_len], dtype) + pad\n",
        "\n",
        "    for i in range(len(lines)):\n",
        "        line_ix = [token_to_id[c] for c in lines[i]]\n",
        "        lines_ix[i, :len(line_ix)] = line_ix\n",
        "        \n",
        "    if not batch_first: # convert [batch, time] into [time, batch]\n",
        "        lines_ix = np.transpose(lines_ix)\n",
        "\n",
        "    return lines_ix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6LjFvFYxNrA"
      },
      "source": [
        "#Example: cast 4 random names to matrices, pad with zeros\n",
        "print('\\n'.join(lines[::2000]))\n",
        "print(to_matrix(lines[::2000]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pt_3MvhXlAf0"
      },
      "source": [
        "tokens[50:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGUUTItHg0C5"
      },
      "source": [
        "# Recurrent neural network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCq_oQ4lkPPe"
      },
      "source": [
        "Recurrent Neural Network is neural networks with the internal state (memory), which are usually used to process step-by-step sequences of inputs.\n",
        "\n",
        "In other neural networks, all the inputs are independent of each other. But in RNN, all the inputs are related to each other and every input can take influence on the internal state.\n",
        "\n",
        "RNN are tradictionally used for tasks which need of processing the sequence, i.e speach-to-text, text-to-text (translation), classification and regression of texts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0MYYU6rkPPt"
      },
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1AFEn3FI3cSxeb5LKOyxWz1aIHmCVmWar' width=680px>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTLD-BeukPPt"
      },
      "source": [
        "Update rule for vector $h$:\n",
        "\n",
        "$h_t = tanh(h_{t-1}W_h + x_t W_x)$\n",
        "\n",
        "**Note**: we can use another nonlinearity except tanh."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRsgx9TXxNrJ"
      },
      "source": [
        "\n",
        "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
        "<img src=\"https://github.com/yandexdataschool/Practical_RL/blob/spring19/week07_%5Brecap%5D_rnn/rnn.png?raw=1\" width=480>\n",
        "\n",
        "Since we're training a language model, there should also be:\n",
        "* An embedding layer that converts character id x_t to a vector.\n",
        "* An output layer that predicts probabilities of next phoneme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyPI4I1exNrO"
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharRNNCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the scheme above as torch module\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"\n",
        "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
        "        We'll call it repeatedly to produce the whole sequence.\n",
        "        \n",
        "        :param x: batch of character ids, int64[batch_size]\n",
        "        :param h_prev: previous rnn hidden states, float32 matrix [batch, rnn_num_units]\n",
        "        \"\"\"\n",
        "        # get vector embedding of x\n",
        "        x_emb = self.embedding(x)\n",
        "        \n",
        "        # compute next hidden state using self.rnn_update\n",
        "        # hint: use torch.cat(..., dim=...) for concatenation\n",
        "        h_next = ###YOUR CODE HERE\n",
        "       \n",
        "        \n",
        "        assert h_next.size() == h_prev.size()\n",
        "        \n",
        "        #compute logits for next character probs\n",
        "        logits = self.rnn_to_logits(h_next)###YOUR CODE\n",
        "        return h_next, F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFhNjoG5xNrV"
      },
      "source": [
        "char_rnn = CharRNNCell()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRG9DwU4xNrk"
      },
      "source": [
        "### RNN loop\n",
        "\n",
        "Difference between cell and layer\n",
        "\n",
        "*(I.e. between RNNCell and RNN)*\n",
        "\n",
        "* A recurrent layer contains a cell object, commands the cell and performs the actual recurrent calculations. \n",
        "* The cell contains the core code for the calculations of each step.\n",
        "\n",
        "Once we've defined a single RNN step, we can apply it in a loop to get predictions on each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wZV18EMxNro"
      },
      "source": [
        "def rnn_loop(char_rnn, batch_ix):\n",
        "    \"\"\"\n",
        "    Computes log P(next_character) for all time-steps in lines_ix\n",
        "    :param lines_ix: an int32 matrix of shape [batch, time], output of to_matrix(lines)\n",
        "    \"\"\"\n",
        "    batch_size, max_length = batch_ix.size()\n",
        "    hid_state = char_rnn.initial_state(batch_size)\n",
        "    logprobs = []\n",
        "\n",
        "    for x_t in batch_ix.transpose(0,1):\n",
        "        hid_state, logp_next = # YOU CODE HERE  # <-- here we call your one-step code\n",
        "        logprobs.append(logp_next)\n",
        "        \n",
        "    return torch.stack(logprobs, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyeW8FGTxNry"
      },
      "source": [
        "batch_ix = to_matrix(lines[:5])\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "cel_obj = CharRNNCell()\n",
        "\n",
        "logp_seq = rnn_loop(cel_obj, batch_ix)\n",
        "\n",
        "assert torch.max(logp_seq).data.numpy() <= 0\n",
        "assert tuple(logp_seq.size()) ==  batch_ix.shape + (num_tokens,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4-dWabbxNr6"
      },
      "source": [
        "### Likelihood and gradients\n",
        "\n",
        "We can now train our neural network to minimize crossentropy (maximize log-likelihood) with the actual next tokens.\n",
        "\n",
        "To do so in a vectorized manner, we take `batch_ix[:, 1:]` - a matrix of token ids shifted i step to the left so i-th element is acutally the \"next token\" for i-th prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLHf-Aq8xNr9"
      },
      "source": [
        "predictions_logp = logp_seq[:, :-1]\n",
        "print (predictions_logp.shape)\n",
        "actual_next_tokens = batch_ix[:, 1:]\n",
        "print (actual_next_tokens.shape, actual_next_tokens)\n",
        "print (actual_next_tokens[:,:,None].shape)\n",
        "\n",
        "logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n",
        "\n",
        "loss = -logp_next.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqUWZX3mRLd6"
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaCMB9JPxNsK"
      },
      "source": [
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vQT0t-qxNsa"
      },
      "source": [
        "for w in char_rnn.parameters():\n",
        "    assert w.grad is not None and torch.max(torch.abs(w.grad)).data.numpy() != 0, \\\n",
        "        \"Loss is not differentiable w.r.t. a weight with shape %s. Check forward method.\" % (w.size(),)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gxi9BhJxNsn"
      },
      "source": [
        "### The training loop\n",
        "\n",
        "We train our char-rnn exactly the same way we train any deep learning model: by minibatch sgd.\n",
        "\n",
        "The only difference is that this time we sample strings, not images or sound."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjcJR_1cxNsq"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from random import sample\n",
        "\n",
        "char_rnn = CharRNNCell()\n",
        "opt = torch.optim.Adam(char_rnn.parameters())\n",
        "history = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6STFsVgsxNsy"
      },
      "source": [
        "\n",
        "for i in range(10000):\n",
        "    batch_ix = to_matrix(sample(lines, 32), max_len=MAX_LENGTH)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    \n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "    \n",
        "    # compute loss\n",
        "    #<YOUR CODE>\n",
        "    opt.zero_grad()\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "    logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n",
        "    loss = -logp_next.mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    #loss = ###YOUR CODE\n",
        "    \n",
        "    # train with backprop\n",
        "    #<YOUR CODE>\n",
        "    \n",
        "    history.append(loss.data.numpy())\n",
        "    if (i+1)%100==0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kae9COy8xNtB"
      },
      "source": [
        "### RNN: sampling\n",
        "Once we've trained our network a bit, let's get to actually generating stuff. \n",
        "All we need is the single rnn step function you have defined in `char_rnn.forward`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piuEtqYexNtG"
      },
      "source": [
        "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
        "    '''\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "                        smaller temperature converges to the single most likely output\n",
        "    '''\n",
        "    \n",
        "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
        "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
        "    hid_state = char_rnn.initial_state(batch_size=1)\n",
        "    \n",
        "    #feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
        "    \n",
        "    #start generating\n",
        "    for _ in range(max_length - len(seed_phrase)):\n",
        "        hid_state, logp_next = char_rnn(x_sequence[:, -1], hid_state)\n",
        "        p_next = F.softmax(logp_next / temperature, dim=-1).data.numpy()[0]\n",
        "        \n",
        "        # sample next token and push it back into x_sequence\n",
        "        next_ix = np.random.choice(num_tokens,p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "        \n",
        "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLx2EBLJxNtY"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzbIA1WPxNtg"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn, seed_phrase=' Covid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwqUmy_pxNtp"
      },
      "source": [
        "### Try it out!\n",
        "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
        "\n",
        "* Novels/poems/songs of your favorite author\n",
        "* News titles/clickbait titles\n",
        "* Source code of Linux or Tensorflow\n",
        "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
        "* Melody in notes/chords format\n",
        "* Ikea catalog titles\n",
        "* Pokemon names\n",
        "* Cards from Magic, the Gathering / Hearthstone\n",
        "\n",
        "If you're willing to give it a try, here's what you wanna look at:\n",
        "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
        "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
        "* Make sure MAX_LENGTH is adjusted for longer datasets. There's also a bonus section about dynamic RNNs at the bottom.\n",
        "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
        "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
        "\n",
        "__Good hunting!__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BgbahGE0xNtw"
      },
      "source": [
        "### More seriously\n",
        "\n",
        "What we just did is a manual low-level implementation of RNN. While it's cool, i guess you won't like the idea of re-writing it from scratch on every occasion. \n",
        "\n",
        "As you might have guessed, torch has a solution for this. To be more specific, there are two options:\n",
        "* `nn.RNNCell(emb_size, rnn_num_units)` - implements a single step of RNN just like you did. Basically concat-linear-tanh\n",
        "* `nn.RNN(emb_size, rnn_num_units` - implements the whole rnn_loop for you.\n",
        "\n",
        "There's also `nn.LSTMCell` vs `nn.LSTM`, `nn.GRUCell` vs `nn.GRU`, etc. etc.\n",
        "\n",
        "In this example we'll rewrite the char_rnn and rnn_loop using high-level rnn API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uewn3r3xNtz"
      },
      "source": [
        "class CharRNNLoop(nn.Module):\n",
        "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n",
        "        super(self.__class__, self).__init__()\n",
        "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
        "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_seq, _ = self.rnn(self.emb(x))\n",
        "        next_logits = self.hid_to_logits(h_seq)\n",
        "        next_logp = F.log_softmax(next_logits, dim=-1)\n",
        "        return next_logp\n",
        "    \n",
        "model = CharRNNLoop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls-H-9u6xNt9"
      },
      "source": [
        "# the model applies over the whole sequence\n",
        "batch_ix = to_matrix(sample(lines, 32), max_len=MAX_LENGTH)\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "logp_seq = model(batch_ix)\n",
        "\n",
        "# compute loss. This time we use nll_loss with some duct tape\n",
        "loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n",
        "                  batch_ix[:, :-1].contiguous().view(-1))\n",
        "\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "oG-50QGhxNuG"
      },
      "source": [
        "Here's another example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LTWutHKxNuL"
      },
      "source": [
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CharLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements something like CharRNNCell, but with LSTM\n",
        "    \"\"\"\n",
        "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
        "        super(self.__class__,self).__init__()\n",
        "        self.num_units = rnn_num_units\n",
        "        self.emb = nn.Embedding(num_tokens, embedding_size)\n",
        "        self.lstm = nn.LSTMCell(embedding_size, rnn_num_units)\n",
        "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
        "        \n",
        "    def forward(self, x, prev_state):\n",
        "        (prev_h, prev_c) = prev_state\n",
        "        (next_h, next_c) = self.lstm(self.emb(x), (prev_h, prev_c))\n",
        "        logits = self.rnn_to_logits(next_h)\n",
        "        \n",
        "        return (next_h, next_c), F.log_softmax(logits, -1)\n",
        "    \n",
        "    def initial_state(self, batch_size):\n",
        "        \"\"\" LSTM has two state variables, cell and hid \"\"\"\n",
        "        return torch.zeros(batch_size, self.num_units), torch.zeros(batch_size, self.num_units)\n",
        "    \n",
        "char_lstm = CharLSTMCell()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR02vVMDxNuS"
      },
      "source": [
        "# the model applies over the whole sequence\n",
        "batch_ix = to_matrix(sample(lines, 32), max_len=MAX_LENGTH)\n",
        "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "\n",
        "logp_seq = rnn_loop(char_lstm, batch_ix)\n",
        "\n",
        "# compute loss. This time we use nll_loss with some duct tape\n",
        "loss = F.nll_loss(logp_seq[:, 1:].contiguous().view(-1, num_tokens), \n",
        "                  batch_ix[:, :-1].contiguous().view(-1))\n",
        "\n",
        "loss.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96JpECwWwGlq"
      },
      "source": [
        "for i in range(10000):\n",
        "    batch_ix = to_matrix(sample(lines, 32), max_len=MAX_LENGTH)\n",
        "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
        "    \n",
        "    logp_seq = rnn_loop(char_rnn, batch_ix)\n",
        "    \n",
        "    # compute loss\n",
        "    #<YOUR CODE>\n",
        "    opt.zero_grad()\n",
        "    predictions_logp = logp_seq[:, :-1]\n",
        "    actual_next_tokens = batch_ix[:, 1:]\n",
        "\n",
        "    logp_next = torch.gather(predictions_logp, dim=2, index=actual_next_tokens[:,:,None])\n",
        "    loss = -logp_next.mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    \n",
        "    #loss = ###YOUR CODE\n",
        "    \n",
        "    # train with backprop\n",
        "    #<YOUR CODE>\n",
        "    \n",
        "    history.append(loss.data.numpy())\n",
        "    if (i+1)%100==0:\n",
        "        clear_output(True)\n",
        "        plt.plot(history,label='loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb14nzs_wYDD"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(generate_sample(char_rnn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eVtel6_xNuX"
      },
      "source": [
        "__Bonus quest: __ implement a model that uses 2 LSTM layers (the second lstm uses the first as input) and train it on your data."
      ]
    }
  ]
}